# Global imports
import os
import re
import argparse
import json
from pathlib import Path
import pandas as pd
from Bio import SeqIO
from datetime import datetime

# Script information - Written in Python 3.9.12 - May 2023
__author__ = "Guillermo Carrillo Martin"
__maintainer__ = "Guillermo Carrillo Martin"
__email__ = "guillermo.carrillo@upf.edu"

"""
This script generates metadata files with information about a multi-fasta protein 
database outputed from the uniparc_download.py script. The software only generates 
the "genes_NOT_retrieved.csv" file if a gene list has been specified. It also might combine
the information present within the database with the information present in the JSON files
generated during the download step (if --no-ignore-json). These JSON files contain the 
repositories, species, and TaxID metadata of each record in the database, as there might be
more than one value per record in these features.

Six (or seven if gene list) metadata files are generated by this script:

1. summary.txt; A text file with a summary of all the metadata information retrieved 
   from the multi-fasta database. It also shows the paths to all the other files.

2. genes_NOT_retrieved.txt; A text file that contains the genes not retrieved. This
   file is only created if the list of genes used to build the multi-fasta is 
   specified.

3. records_info.csv; A CSV file that contains all the information present in each 
   record header. See the README.md file for a detailed description of the header 
   information.

4. genes_retrieved.csv; A CSV file that contains the number of genes retrieved.

5. species_retrieved.csv; A CSV file that contains the number of species retrieved, 
   indicating the scientific name and the TaxID.

6. repositories_employed.csv; A CSV file that contains the number of the different
   sources (repositories) used to build the multi-fasta. 

7. species_genes.csv; A CSV file that shows the number of genes retrieved for each 
   species.
"""
def main():

  print("# Generating metadata information")
  database_path, metadata_folder_path, tax_id, gene_list_path, do_ignore_json = parser()

  # Create a folder to store metadata information
  if not os.path.exists(metadata_folder_path):    
      os.mkdir(metadata_folder_path)

  # Create a dataframe with all the info per record and write it as csv
  repos_metadata_dic = None
  species_metadata_dic = None
  taxid_metadata_dic = None

  if not do_ignore_json:
    json_directory_path = Path(database_path).parent
    
    # If there are not JSON files in the directory, print an error
    if not os.path.exists(f"{json_directory_path}/.repos_metadata.json") or \
       not os.path.exists(f"{json_directory_path}/.species_metadata.json") or \
       not os.path.exists(f"{json_directory_path}/.taxid_metadata.json"):
      print("ERROR: No JSON files found in the directory. Please try --ignore-json.")
      exit(0)

    repos_metadata_dic = json.load(open(f"{json_directory_path}/.repos_metadata.json", 'r'))
    species_metadata_dic = json.load(open(f"{json_directory_path}/.species_metadata.json", 'r'))
    taxid_metadata_dic = json.load(open(f"{json_directory_path}/.taxid_metadata.json", 'r'))

  record_info_df = write_records_info_csv(database_path, metadata_folder_path, repos_metadata_dic, species_metadata_dic, taxid_metadata_dic)
  
  # If a gene list was inputed, write the 'genes_NOT_retrieved' list
  if gene_list_path:
    gene_search_count = write_genes_not_retrieved_txt(record_info_df, gene_list_path, metadata_folder_path)
  elif not gene_list_path:
    gene_search_count = None    
  
  # Write the 'count' csv files
  genes_retrieved_count = write_genes_retrieved_csv(record_info_df, metadata_folder_path)
  repositories_total_count = write_repositories_employed_csv(record_info_df, metadata_folder_path)
  species_total_count = write_species_retrieved_csv(record_info_df, metadata_folder_path)
  write_species_per_gene_csv(record_info_df, metadata_folder_path)
  
  # Write the summary.txt file
  write_summary_txt(database_path, metadata_folder_path, gene_list_path, tax_id, genes_retrieved_count, gene_search_count, repositories_total_count, species_total_count)

def parser():
    """
    This function parses the required arguments from the terminal to the python script.
    
    #OUTPUT
    - database_path (string); The path to the input multi-fasta database.
    - metadata_folder_path (string); The path to a folder where the results will be stored.
    - tax_id (integer); The TaxID number employed to construct the multi-fasta database.
    - gene_list_path (string); The path to the gene list employed to construct the multi-fasta.
      database. If no gene list is specified, the variable will be assigned as None.
    - do_ignore_json (boolean); A boolean indicator to indicate if the
      'ignore JSON files' process happens.
    """
  
    parser = argparse.ArgumentParser(description="A script that creates metadata information for a database created by uniparc_download.py")
    parser.add_argument("--input-path", dest="input_path", type=str, help="The path to the input database", required=True, nargs=1)
    parser.add_argument("--output-path", dest="output_path", type=str, help="The path to write the metadata folder (default: working directory)", required=False, default=["."], nargs=1)
    parser.add_argument("--output-folder-name", dest="output_folder_name", type=str, help="The name of the folder storing the metadata (default: metadata)", required=False, default=["metadata"], nargs=1)
    parser.add_argument("--tax-id", dest="TaxID", type=int, help="The TaxID number employed to construct the multi-fasta database", required=True, nargs=1)
    parser.add_argument("--genes", dest="gene_list", type=str, help="The path to the list of genes (not mandatory)", required=False, nargs=1)
    parser.add_argument("--ignore-json", dest="ignore_json", action=argparse.BooleanOptionalAction, help="Ignore JSON files containing extra metadata per each record (default: False; --no-ignore-json)", default=False, required=False)

    args = parser.parse_args()
  
    output_path = os.path.realpath(args.output_path[0])
    output_folder_name = args.output_folder_name[0]
    
    metadata_folder_path = f"{output_path}/{output_folder_name}"
    database_path = os.path.realpath(args.input_path[0])
    tax_id = args.TaxID[0]

    if args.gene_list:
        gene_list_path = os.path.realpath((args.gene_list[0]))
    elif not args.gene_list:
        gene_list_path = None

    do_ignore_json = args.ignore_json

    return database_path, metadata_folder_path, tax_id, gene_list_path, do_ignore_json

def retrieve_tag(regular_expression, header):
    """
    This function extracts a substring from a fasta header using a regular expression.
    If the expression is not fullfilled, it would return an empty string.
    
    #INPUT
    - regular_expression (string); The regular expression to select the substring.
    - header (string); A fasta header, formatted from the uniparc_download.py script.
    #OUTPUT
    - tag (String); The retrieved substring from the fasta header.
    """
    # Retrieve the tag
    tag_match = re.search(regular_expression, header)
    if tag_match:
        tag = tag_match.group(1)
    
    # If no match, retrieves the string "NA"
    elif not tag_match:
        tag = "no gene"

    return tag

def write_records_info_csv(database_path, metadata_folder_path, repos_metadata_dic, species_metadata_dic, taxid_metadata_dic):
    """
    This function retrieves all the information present in each record header and writes it 
    as a CSV file. It also returns a dataframe with all the information present in each record header.
    The information retrieved includes the UPI identifier, repository, gene, species, TaxID,
    last update date, and sequence version. If do_ignore_json = False, the information of
    repositories, species and TaxIDs is retrieved from the JSON files generated during the download step.
    
    #INPUT
    - database_path (string); The path to the multi-fasta database.
    - metadata_folder_path (string); The path to a folder where the results will be stored.
    - repos_metadata_dic (dictionary); A dictionary containing the repositories metadata
      for each record in the database. The key is the UPI identifier and the value is a list
      of repositories.
    - species_metadata_dic (dictionary); A dictionary containing the species metadata
      for each record in the database. The key is the UPI identifier and the value is a list
      of species.
    - taxid_metadata_dic (dictionary); A dictionary containing the TaxID metadata
      for each record in the database. The key is the UPI identifier and the value is a list
      of TaxIDs.
    #OUTPUT
    - record_info_df (pd.DataFrame); A dataframe containing all the information present 
      in each record header.
    #WRITE OUTPUT
    - records_info.csv; A CSV file that contains all the information present in each 
      record header. See the README.md file for a detailed description of the header 
      information.
    """
    record_info_list = []
    
    # Retrieve each tag per fasta header and store the result in a list of lists
    for record in SeqIO.parse(database_path, "fasta"):
        upi_identifier = retrieve_tag(r"\|(UPI[0-9A-Z]{10})", record.description) # UPI\d{10}\w* or UPI[0-9A-Z]{10}
        
        if repos_metadata_dic:
          repository_list = repos_metadata_dic[upi_identifier]
          repository = ';'.join(repository_list)
        else:    
          repository = retrieve_tag(r"^([^|]+)", record.description)
        
        gene = retrieve_tag(r"GN=(.*?)\sSV=", record.description)        
        
        if species_metadata_dic:
          species_list = species_metadata_dic[upi_identifier]
          species = ';'.join(species_list)
        else:    
          species = retrieve_tag(r"OS=(.*?)\sOX=", record.description)
        
        if taxid_metadata_dic:
          taxid_list = taxid_metadata_dic[upi_identifier]
          taxid = ';'.join(str(taxid) for taxid in taxid_list)
        else:
          taxid = retrieve_tag(r"OX=(\w+)", record.description)
        
        date = retrieve_tag(r"\|([\d-]+)", record.description)
        
        version = retrieve_tag(r"SV=(\d+)", record.description)   

        header_info = [upi_identifier, repository, gene, species, taxid, date, version]
        record_info_list.append(header_info)

    # Generate the pandas dataframe and write the csv
    colnames = ["Unic Identifier", "Repository", "Gene", "Species", "TaxID", "Last update", "Sequence version"]
    record_info_df = pd.DataFrame(record_info_list, columns=colnames)
    record_info_df.to_csv(f"{metadata_folder_path}/records_info.csv", index=False)

    return record_info_df

def write_genes_not_retrieved_txt(record_info_df, gene_list_path, metadata_folder_path):
    """
    This function outputs a text file with the genes not retrieved after the protein
    download. It also returns the number of genes that were found.
    
    #INPUT
    - record_info_df (pd.DataFrame); A dataframe containing all the information present 
      in each record header.
    - gene_list_path (string); The path to the gene list employed to construct the multi-fasta 
      database.
    - metadata_folder_path (string); The path to a folder where the results will be stored.
    #OUTPUT
    - gene_search_count (integer); The number of genes set in the query parameters to focus 
      the protein download.
    #WRITE OUTPUT
    - genes_NOT_retrieved.txt; A text file with all the genes not retrieved after the protein
      download.
    """
    # Append in a python list all the genes in gene_list.txt
    gene_search_list = []
    with open(gene_list_path, "rt") as gene_list_file: #Open the gene list file
        for gene in gene_list_file:
            gene_search_list.append(gene.strip().replace("\n", "")) #Append the genes in the file to a list

    # Append in a python the genes found after the protein download
    gene_found_list = list(record_info_df["Gene"].unique())

    # Compare the gene_found_list with the gene_search_list and create genes_not_found_list
    gene_not_found_ls = []
    for gene in gene_search_list:
        if gene not in gene_found_list:
            gene_not_found_ls.append(gene)

    # Write the genes not retrieved in a text file
    with open(metadata_folder_path + "/genes_NOT_retrieved.txt", "wt") as genes_not_retrieved:
        for gene in gene_not_found_ls:
            genes_not_retrieved.write(gene + "\n")
    
    # Count the number of different genes found after the protein download
    gene_search_count = len(gene_search_list)

    return gene_search_count

def write_genes_retrieved_csv(record_info_df, metadata_folder_path):
    """
    This function returns the number of genes found after the protein download. It also writes, 
    as a CSV file, the number of times each gene has been retrieved.
    
    #INPUT
    - record_info_df (pd.DataFrame); A dataframe containing all the information present 
      in each record header.
    - metadata_folder_path (string); The path to a folder where the results will be stored.
    #OUTPUT
    - genes_retrieved_count (integer); The number of different genes found after the download.
    #WRITE OUTPUT
    - genes_retrieved.csv; A CSV file with the number of times each different gene has been 
      found.
    """
    # Count, in a Panda series, the time each "Gene" appears in the database
    gene_retrieved_count_sr = record_info_df["Gene"].value_counts()
    gene_retrieved_count_sr.to_csv(metadata_folder_path + "/genes_retrieved.csv", index_label="Gene", header=["Count"])

    # Count the number of different genes
    if "no gene" in gene_retrieved_count_sr:
      gene_retrieved_count_sr = gene_retrieved_count_sr.drop("no gene")
    genes_retrieved_count = gene_retrieved_count_sr.count()

    return genes_retrieved_count

def write_repositories_employed_csv(record_info_df, metadata_folder_path):
    """
    This function returns the number of repositories found after the protein download. 
    It also writes, as a CSV file, the number of times each repository has been retrieved.
    Since each record can be associated with multiple repositories, the count may exceed 
    the total count of records.
    
    #INPUT
    - record_info_df (pd.DataFrame); A dataframe containing all the information present 
      in each record header.
    - metadata_folder_path (string); The path to a folder where the results will be stored.
    #OUTPUT
    - repositories_total_count (integer); The number of different repositories found.
    #WRITE OUTPUT
    - repositories_employed.csv; A CSV file with the number of times each different repository
      has been found.
    """

    local_record_info_df = record_info_df.copy() # Create a copy of the dataframe to avoid modifying the original one

    # Split the semicolon-separated values into different rows
    local_record_info_df["Repository"] = local_record_info_df["Repository"].str.split("; ")    
    local_record_info_df = local_record_info_df.explode("Repository")

    # Count the occurrences of each "Repository"
    repositories_count_sr = local_record_info_df["Repository"].value_counts()
    repositories_count_sr.to_csv(metadata_folder_path + "/repositories_employed.csv", index_label="Repository", header=["Count"])

    # Count the number of different repositories
    repositories_total_count = repositories_count_sr.count()

    return repositories_total_count

def write_species_retrieved_csv(record_info_df, metadata_folder_path):
    """
    This function returns the number of species found after the protein download. 
    It also writes, as a CSV file, the number of times each species has been retrieved,
    indicating also the corresponding TaxID. Since each record can be associated with 
    multiple species, the count may exceed the total count of records.
    
    #INPUT
    - record_info_df (pd.DataFrame); A dataframe containing all the information present 
      in each record header.
    - metadata_folder_path (string); The path to a folder where the results will be stored.
    #OUTPUT
    - species_total_count (integer); The number of different species found.
    #WRITE OUTPUT
    - species_retrieved.csv; A CSV file with the number of times each different species
      has been found, with its corresponding TaxID.
    """

    local_record_info_df = record_info_df.copy() # Create a copy of the dataframe to avoid modifying the original one

    # Split the semicolon-separated values into different rows
    local_record_info_df["Species"] = local_record_info_df["Species"].str.split(";")
    local_record_info_df["TaxID"] = local_record_info_df["TaxID"].str.split(";")
    local_record_info_df = local_record_info_df.explode(["Species", "TaxID"])

    # Count, in a Panda series, the time each "Species-TaxID" appears in the database
    species_count_sr = local_record_info_df[["Species", "TaxID"]].value_counts()
    species_count_sr.to_csv(metadata_folder_path + "/species_retrieved.csv", index_label=["Species", "TaxID"], header=["Count"])

    # Count the number of different species
    species_total_count = species_count_sr.count()

    return species_total_count

def write_species_per_gene_csv(record_info_df, metadata_folder_path):
    """
    This function writes, as a CSV file, the number of times each gene has been found in each species.
    In other words, it counts the number of times per each gene-species combination. Since each record 
    can be associated with multiple species, the count may exceed the total count of records.
    
    #INPUT
    - record_info_df (pd.DataFrame); A dataframe containing all the information present 
      in each record header.
    - metadata_folder_path (string); The path to a folder where the results will be stored.
    #WRITE OUTPUT
    - species_genes.csv; A CSV file with the number of times each different gene-species 
      combination has been found.
    """
    # Split the semicolon-separated values into different rows
    record_info_df["Species"] = record_info_df["Species"].str.split(";")
    record_info_df = record_info_df.explode("Species")
    
    # Calculate the count per each gene and species combination
    species_gene_df = record_info_df.groupby(["Gene", "Species"]).size().reset_index(name="Count")
    
    # Sort and reorder the possition of the columns
    species_gene_df.sort_values(["Species", "Count"], axis=0,ascending=[True, False], inplace=True)
    species_gene_df = species_gene_df.loc[:, ["Species", "Gene", "Count"]] # Reorder the columns

    # Write the dataframe as a csv
    species_gene_df.to_csv(f"{metadata_folder_path}/species_genes.csv", index=False)

def write_summary_txt(database_path, metadata_folder_path, gene_list_path, tax_id, genes_retrieved_count, gene_search_count, repositories_total_count, species_total_count):
  """
  This function writes a summary of all the metadata in a text file. It also writes a serie
  of errors in the file if some conditions are fulfilled, like the absence of gene name in a 
  header. 
  
  #INPUT
  - database_path (string); The path to the multi-fasta database.
  - metadata_folder_path (string); The path to a folder where the results will be stored.
  - gene_list_path (string); The path to the gene list employed to construct the multi-fasta.
  - tax_id (integer); The TaxID number employed to construct the multi-fasta database.
  - genes_retrieved_count (integer); The number of different genes found.
  - gene_search_count (integer); The number of genes to focus the protein download.
  - repositories_total_count (integer); The number of different repositories found.
  - species_total_count (integer); The number of different species found.
  #WRITE OUPUT
  - summary.txt; A text file with a summary of all the metadata information retrieved 
    from the multi-fasta database. It also shows the paths to all the other files.
  """
  with open(metadata_folder_path + "/summary.txt", "wt") as metadata:

    # Metadata general information 
    metadata.write(f"## METADATA for the database {database_path}\n")
    metadata.write(f"TaxID: {tax_id}\n")
    
    if gene_list_path:
        metadata.write(f"Gene list: {gene_list_path}\n")
    
    # Current time and date
    current_date_time = datetime.now()
    date_time = current_date_time.strftime("%d/%m/%Y - %H:%M:%S")
    metadata.write(f"{date_time}\n" + "\n")

    # Warnings
    warnings = 0
    record_num = os.popen(f"grep -E -c '^>' {database_path}").read()
    metadata.write("# WARNINGS\n")

    record_with_gene = os.popen(f"cat {database_path} | grep -E '^>' | grep -c 'GN='").read()
    if record_num != record_with_gene: # Not all the headers have a gene name
        warnings += 1
        metadata.write("Check gene names (GN=)\n")
    
    record_with_taxname = os.popen(f"cat {database_path} | grep -E '^>' | grep -c 'OS='").read()
    if record_num != record_with_taxname: # Not all the headers have a species tag
        warnings += 1
        metadata.write("Check taxa names (OS=)\n")

    record_with_taxid = os.popen(f"cat {database_path} | grep -E '^>' | grep -c 'OX='").read()
    if record_num != record_with_taxid: # Not all the headers have a TaxID
        warnings += 1
        metadata.write("Check taxIDs (OX=)\n")

    if gene_list_path:
        if genes_retrieved_count != gene_search_count: # Not all the search genes have been retrieved
            warnings += 1
            metadata.write(f"{(gene_search_count) - (genes_retrieved_count)} genes not found\n")

    if warnings == 0:
        metadata.write("No warnings\n")

    # Disclaimer
    metadata.write("\n# DISCLAIMER\n")
    metadata.write("Each record can be associated with multiple repositories and/or species. As a result, the total number of repositories or species may exceed the number of records, and should not be interpreted as a one-to-one correspondence\n")

    # Stats about the protein records
    metadata.write("\n# Stats about the protein records\n" + \
                    "records: {}".format(record_num) + \
                    "records_with_gene: {}/{}".format(record_with_gene[:-1],record_num) + \
                    "records_with_taxname: {}/{}".format(record_with_taxname[:-1],record_num) + \
                    "records_with_taxid: {}/{}".format(record_with_taxid[:-1],record_num) + 
                    "all_records_path: {}\n".format(metadata_folder_path + "/records_info.csv"))

    # Stats about the employed repositories
    metadata.write("\n# Stats about the employed repositories\n" + \
                    "repositories_employed: {}\n".format(repositories_total_count) + \
                    "repositories_employed_path: {}\n\n".format(metadata_folder_path + "/repositories_employed.csv"))

    # Stats about the genes found
    if gene_list_path:            
        metadata.write("# Stats about the genes found\n" + \
                        "genes_retrieved: {}/{}\n".format(genes_retrieved_count,gene_search_count) + \
                        "genes_retrieved_path: {}\n".format(metadata_folder_path + "/genes_retrieved.csv") + \
                        "genes_not_retrieved_path: {}\n".format(metadata_folder_path + "/genes_NOT_retrieved.txt"))
    
    elif not gene_list_path:
        metadata.write("# Stats about the genes found\n" + \
                        "genes_retrieved: {}\n".format(genes_retrieved_count) + \
                        "genes_retrieved_path: {}\n".format(metadata_folder_path + "/genes_retrieved.csv"))

    #  Stats about the species in the database
    metadata.write("\n# Stats about the species in the database\n" + \
                    "species_retrieved: {}\n".format(species_total_count) + \
                    "species_retrieved_path: {}\n".format(metadata_folder_path + "/species_retrieved.csv") + \
                    "species_and_genes_path: {}\n".format(metadata_folder_path + "/species_genes.csv"))

main()