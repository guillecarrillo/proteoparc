{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old API download\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "#0. Variables needed (list of genes, tax id and the database file name)\n",
    "gene_list_path = \"gene_list/enamelome_JK.txt\"\n",
    "db_name = \"proboscidea_db.fasta\"\n",
    "tax_id = 9779\n",
    "\n",
    "#1. Create the gene list\n",
    "gene_list = []\n",
    "\n",
    "with open(gene_list_path, \"rt\") as gene_list_file:\n",
    "    for gene in gene_list_file:\n",
    "        gene_list.append(gene[0:-1])\n",
    "\n",
    "#2. Create the taxID list of species contained in the taxID\n",
    "\n",
    "import requests\n",
    "import os\n",
    "\n",
    "tax_id = 9779\n",
    "url_tax_id_descendent = \"https://rest.uniprot.org/taxonomy/stream?format=list&query=%28ancestor%3A\" + str(tax_id) + \"%29\"\n",
    "\n",
    "tax_id_list = []\n",
    "\n",
    "with open(\"./taxID.txt\", \"wb\") as tax_id_descendent_file:\n",
    "            \n",
    "    #Extract each protein record for the search\n",
    "    with requests.get(url_tax_id_descendent, stream=True) as request_tax_id_descedent:\n",
    "        request_tax_id_descedent.raise_for_status()\n",
    "\n",
    "        #Append the protein records to the db fasta file\n",
    "        for id in request_tax_id_descedent.iter_content():\n",
    "            tax_id_descendent_file.write(id)\n",
    "\n",
    "\n",
    "with open(\"./taxID.txt\", \"rt\") as tax_id_file:\n",
    "    for id in tax_id_file:\n",
    "        tax_id_list.append(int(id[0:-1]))\n",
    "\n",
    "os.remove(\"./taxID.txt\") \n",
    "\n",
    "#taxID organism name\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "tax_id = 9779\n",
    "url_org_name = \"https://rest.uniprot.org/taxonomy/stream?format=json&query=%28\" + str(tax_id) + \"%29\"\n",
    "\n",
    "with open(\"./org_name.json\", \"wb\") as org_name_json:\n",
    "            \n",
    "    #Extract each protein record for the search\n",
    "    with requests.get(url_org_name, stream=True) as request_org_name:\n",
    "        request_org_name.raise_for_status()\n",
    "\n",
    "        #Append the protein records to the db fasta file\n",
    "        for chunk in request_org_name.iter_content():\n",
    "            org_name_json.write(chunk)\n",
    "\n",
    "with open(\"./org_name.json\", \"rt\") as org_name_json:\n",
    "    org_name_dic = json.load(org_name_json)\n",
    "\n",
    "    org_name = org_name_dic[\"results\"][0][\"scientificName\"]\n",
    "\n",
    "os.remove(\"./org_name.json\")\n",
    "\n",
    "#3. Convert JSON format to FASTA\n",
    "\n",
    "def json_to_fasta(json_path, fasta_path):\n",
    "    with open(json_path, \"rt\") as prot_json:\n",
    "        prot_dic = json.load(prot_json) #Convert json format into a dictionary\n",
    "\n",
    "    with open(fasta_path, \"at\") as prot_fasta: #Write in a text file the fasta records\n",
    "        for record in prot_dic[\"results\"]:\n",
    "            \n",
    "            #Construct the header\n",
    "            header = \">\" + record[\"uniParcCrossReferences\"][0][\"database\"] + \"|\" + record[\"uniParcId\"] + \" \" + \\\n",
    "            record[\"uniParcCrossReferences\"][0][\"proteinName\"] + \" \"\n",
    "            \n",
    "            if record[\"uniParcCrossReferences\"][0][\"organism\"][\"taxonId\"] not in tax_id_list:\n",
    "                header += \"OS=\" + org_name + \" \" + \\\n",
    "                \"OX=\" + str(tax_id) + \" \"\n",
    "            else:\n",
    "                header += \"OS=\" + record[\"uniParcCrossReferences\"][0][\"organism\"][\"scientificName\"] + \" \" + \\\n",
    "                \"OX=\" + str(record[\"uniParcCrossReferences\"][0][\"organism\"][\"taxonId\"]) + \" \"\n",
    "            \n",
    "            header += \"GN=\" + gene\n",
    "\n",
    "            prot_fasta.write(header + \"\\n\")\n",
    "\n",
    "            #Write the protein sequence in rows of 60 aminoacids\n",
    "            sequence_split = re.findall(\".{1,60}\", record[\"sequence\"][\"value\"])\n",
    "            for row in sequence_split:\n",
    "                prot_fasta.write(row + \"\\n\")\n",
    "\n",
    "    prot_json.close()\n",
    "\n",
    "#4. Programmatic acces to UniProtKb through the UniProt API  \n",
    "\n",
    "#remove the previous database with the same name\n",
    "\n",
    "if os.path.exists(\"proboscidea_db.fasta\"):\n",
    "    os.remove(\"proboscidea_db.fasta\")\n",
    "\n",
    "for gene in gene_list: \n",
    "    #API URL needed to download the proper sequences\n",
    "    url = \"https://rest.uniprot.org/uniparc/stream?compressed=false&format=json&query=%28%28gene%3A\" + gene + \"%29%20AND%20%28taxonomy_id%3A\" + str(tax_id) + \"%29%29\"\n",
    "\n",
    "    #Create the db fasta file to append the protein records\n",
    "    with open(\"./db.json\", \"wb\") as db_json:\n",
    "            \n",
    "            #Extract each protein record for the search\n",
    "            with requests.get(url, stream=True) as request:\n",
    "                request.raise_for_status()\n",
    "\n",
    "                #Append the protein records to the db fasta file\n",
    "                for chunk in request.iter_content():\n",
    "                    db_json.write(chunk)\n",
    "            \n",
    "    db_json.close()\n",
    "\n",
    "    json_to_fasta(\"./db.json\", db_name)\n",
    "    os.remove(\"./db.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: No internet connection\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl Kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. Revise el código de las celdas para identificar una posible causa del error. Haga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. Vea el [registro] de Jupyter (command:jupyter.viewOutput) para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# Internet connection\n",
    "\n",
    "import requests\n",
    "\n",
    "def internet_on():\n",
    "    try:\n",
    "        requests.get('http://www.google.com', timeout=5)\n",
    "        return True\n",
    "    except requests.ConnectionError as err: \n",
    "        return False\n",
    "\n",
    "if not internet_on():\n",
    "    print(\"ERROR: No internet connection\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New API downolad full\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# Script information - Written in Python 3.9.12 - April 2023\n",
    "__author__ = \"Guillermo Carrillo Martín\"\n",
    "__maintainer__ = \"Guillermo Carrillo Martín\"\n",
    "__email__ = \"guillermo.carrillo@upf.edu\"\n",
    "\n",
    "\"\"\"\n",
    "This script creates a protein database in fasta format using the UniParc \n",
    "archive, a non redundand database that contains all the proteins annotated \n",
    "using ENSEMBL, NCBI and other resouces. The search is focus for a specific \n",
    "taxonomic group (indicated using the NCBI tax ID) and can be reduced to \n",
    "a certain group of genes, indicated in a list. Other spcecificities, as \n",
    "the descripion of the protein header, can be seen in the README.md file.\n",
    "\"\"\"\n",
    "\n",
    "#0. Variables needed\n",
    "gene_list_path = \"gene_list/enamelome_JK.txt\" # The path to the list of genes\n",
    "db_name = \"proboscidea_db.fasta\" # The name of the database file\n",
    "tax_id = 9779 # The tax ID of the organism used to search in UniParc\n",
    "\n",
    "#1. Create the gene list\n",
    "gene_list = []\n",
    "\n",
    "with open(gene_list_path, \"rt\") as gene_list_file: #Open gene list file\n",
    "    for gene in gene_list_file:\n",
    "        gene_list.append(gene[0:-1]) #Append the genes in the file to a list\n",
    "\n",
    "#2. Create a taxID list of taxonomic descendent species contained in the taxID\n",
    "\n",
    "\"\"\"\n",
    "In Uniprot, all the proteins with the same sequence have the same unic\n",
    "identifier even they belong to different species. So, at the end, if a \n",
    "protein is conserved throught many species it appears in the database as\n",
    "one record (and the proper unic idenifier) with many species associated.\n",
    "Because of this, sometimes the program retrieves in a recorda random specie \n",
    "that is not from our linage. To fix this error, we create a list with all the \n",
    "downstream taxID to evaluate if the association belongs or not to the correct\n",
    "linage. If it founds a missmatch, in the header there will be the following\n",
    "modifications:\n",
    "\n",
    "OS= taxID corresponging name (for example, Proboscidea)\n",
    "OX= taxID number (for example, 9779)\n",
    "\"\"\"\n",
    "\n",
    "# 2.1 Build API link for the taxonomy download in uniprot\n",
    "url_tax_id_descendent = \"https://rest.uniprot.org/taxonomy/stream?format=list&query=%28ancestor%3A\" + str(tax_id) + \"%29\"\n",
    "tax_id_list = []\n",
    "\n",
    "# Download the taxID's in a temporal text file\n",
    "with open(\"./taxID.txt\", \"wb\") as tax_id_descendent_file:\n",
    "    \n",
    "    # Extract each taxID from the search        \n",
    "    with requests.get(url_tax_id_descendent, stream=True) as request_tax_id_descedent:\n",
    "        request_tax_id_descedent.raise_for_status()\n",
    "\n",
    "        # Append the taxID's in the text file\n",
    "        for id in request_tax_id_descedent.iter_content():\n",
    "            tax_id_descendent_file.write(id)\n",
    "\n",
    "# Open the taxID's file and append them into a list \n",
    "with open(\"./taxID.txt\", \"rt\") as tax_id_file:\n",
    "    for id in tax_id_file:\n",
    "        tax_id_list.append(int(id[0:-1]))\n",
    "\n",
    "# Remove the temporal text file with the taxID's\n",
    "os.remove(\"./taxID.txt\") \n",
    "\n",
    "# 2.2 taxID organism name\n",
    "\n",
    "tax_id = 9779\n",
    "url_org_name = \"https://rest.uniprot.org/taxonomy/stream?format=json&query=%28\" + str(tax_id) + \"%29\"\n",
    "\n",
    "with open(\"./org_name.json\", \"wb\") as org_name_json:\n",
    "            \n",
    "    #Extract each protein record for the search\n",
    "    with requests.get(url_org_name, stream=True) as request_org_name:\n",
    "        request_org_name.raise_for_status()\n",
    "\n",
    "        #Append the protein records to the db fasta file\n",
    "        for chunk in request_org_name.iter_content():\n",
    "            org_name_json.write(chunk)\n",
    "\n",
    "with open(\"./org_name.json\", \"rt\") as org_name_json:\n",
    "    org_name_dic = json.load(org_name_json)\n",
    "\n",
    "    org_name = org_name_dic[\"results\"][0][\"scientificName\"]\n",
    "\n",
    "os.remove(\"./org_name.json\")\n",
    "\n",
    "#3. Convert JSON format to FASTA\n",
    "\n",
    "def json_to_fasta(json_path, fasta_path, gene_name):\n",
    "    with open(json_path, \"rt\") as prot_json:\n",
    "        prot_dic = json.load(prot_json) #Convert json format into a dictionary\n",
    "\n",
    "    with open(fasta_path, \"at\") as prot_fasta: #Write in a text file the fasta records\n",
    "        for record in prot_dic[\"results\"]:\n",
    "            \n",
    "            #Construct the header\n",
    "            header = \">\" + record[\"uniParcCrossReferences\"][0][\"database\"] + \"|\" + record[\"uniParcId\"] + \"|\" + \\\n",
    "            record[\"uniParcCrossReferences\"][0][\"lastUpdated\"]+ \" \" + \\\n",
    "            record[\"uniParcCrossReferences\"][0][\"proteinName\"] + \" \"\n",
    "            \n",
    "            if record[\"uniParcCrossReferences\"][0][\"organism\"][\"taxonId\"] not in tax_id_list:\n",
    "                header += \"OS=\" + org_name + \" \" + \\\n",
    "                \"OX=\" + str(tax_id) + \" \"\n",
    "            else:\n",
    "                header += \"OS=\" + record[\"uniParcCrossReferences\"][0][\"organism\"][\"scientificName\"] + \" \" + \\\n",
    "                \"OX=\" + str(record[\"uniParcCrossReferences\"][0][\"organism\"][\"taxonId\"]) + \" \"\n",
    "            \n",
    "            header += \"GN=\" + gene_name\n",
    "\n",
    "            prot_fasta.write(header + \"\\n\")\n",
    "\n",
    "            #Write the protein sequence in rows of 60 aminoacids\n",
    "            sequence_split = re.findall(\".{1,60}\", record[\"sequence\"][\"value\"])\n",
    "            for row in sequence_split:\n",
    "                prot_fasta.write(row + \"\\n\")\n",
    "\n",
    "    prot_json.close()\n",
    "\n",
    "\n",
    "#4. Programmatic acces to UniProtKb through the UniProt API  \n",
    "\n",
    "#remove the previous database with the same name\n",
    "\n",
    "if os.path.exists(\"proboscidea_db.fasta\"):\n",
    "    os.remove(\"proboscidea_db.fasta\")\n",
    "\n",
    "#Programatic acces setup\n",
    "\n",
    "re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
    "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def get_next_link(headers):\n",
    "    if \"Link\" in headers:\n",
    "        match = re_next_link.match(headers[\"Link\"])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "def get_batch(batch_url):\n",
    "    while batch_url:\n",
    "        response = session.get(batch_url)\n",
    "        response.raise_for_status()\n",
    "        total = response.headers[\"x-total-results\"]\n",
    "        yield response, total\n",
    "        batch_url = get_next_link(response.headers)\n",
    "\n",
    "\n",
    "\n",
    "#Create the db fasta file to append the protein records\n",
    "progress = 0\n",
    "\n",
    "for gene in gene_list:\n",
    "    #API URL needed to download the proper sequences\n",
    "    url = \"https://rest.uniprot.org/uniparc/search?compressed=false&format=json&query=%28%28gene%3A\" + gene + \"%29%20AND%20%28taxonomy_id%3A\" + str(tax_id) + \"%29%29&size=500\"\n",
    "\n",
    "    with open(\"./db.json\", 'w') as f:\n",
    "        for batch, total in get_batch(url):\n",
    "            lines = batch.text.splitlines()\n",
    "            if not progress:\n",
    "                print(lines[0], file=f)\n",
    "            for line in lines[1:]:\n",
    "                print(line, file=f)\n",
    "            progress += len(lines[1:])\n",
    "\n",
    "    json_to_fasta(\"./db.json\", db_name, gene)\n",
    "    os.remove(\"./db.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "\n",
    "#Global imports\n",
    "import re\n",
    "import os\n",
    "\n",
    "#Variables\n",
    "input_file_path = \"proboscidea_db.fasta\"\n",
    "output_file_path = \"proboscidea_db_removed.fasta\"\n",
    "\n",
    "#Remove breaks at the end of the sequence lines\n",
    "def fasta_oneliner(input_path, output_path):\n",
    "    fasta_file = open(input_path, \"rt\")\n",
    "    output = open(output_path, \"wt\")\n",
    "\n",
    "    lin = fasta_file.readline()\n",
    "    output.write(lin)\n",
    "\n",
    "    for line in fasta_file:\n",
    "        match = re.search(r\"^>\",line)\n",
    "        if match:\n",
    "            output.write(\"\\n\" + line)\n",
    "        else:\n",
    "            output.write(line.replace(\"\\n\",\"\"))\n",
    "\n",
    "    fasta_file.close()\n",
    "    output.close()\n",
    "\n",
    "fasta_oneliner(input_file_path, \"./oneliner.fasta\")\n",
    "\n",
    "#Append the records to a nested list [header, sequence]\n",
    "record_list = []\n",
    "\n",
    "with open(\"./oneliner.fasta\", \"rt\") as multifasta_file:\n",
    "    while True:\n",
    "        tag = multifasta_file.readline().strip() #Tag\n",
    "        \n",
    "        if not tag: \n",
    "            break #End of file\n",
    "        \n",
    "        sequence = multifasta_file.readline().strip() #Sequence\n",
    "\n",
    "        record = [tag, sequence]\n",
    "\n",
    "        record_list.append(record)\n",
    "\n",
    "#Remove the temporal sequence-oneliner file\n",
    "os.remove(\"./oneliner.fasta\")\n",
    "\n",
    "#Create a new list to remove the sequences\n",
    "record_list_removed = record_list.copy()\n",
    "\n",
    "#Create variables to count the number of removed records\n",
    "records_equal_count = 0 \n",
    "records_contained_count = 0\n",
    "\n",
    "#Remove the duplicates in the list\n",
    "for record1 in record_list_removed:\n",
    "    for record2 in record_list_removed[record_list_removed.index(record1) + 1:]:\n",
    "        if record1[1] == record2[1]:\n",
    "            record_list_removed.remove(record1)\n",
    "            records_equal_count += 1\n",
    "            break\n",
    "\n",
    "        elif record1[1] in record2[1]:\n",
    "            record_list_removed.remove(record1)\n",
    "            records_contained_count += 1\n",
    "            break\n",
    "\n",
    "        elif record2[1] in record1[1]:\n",
    "            record_list_removed.remove(record2)\n",
    "            records_contained_count += 1\n",
    "\n",
    "print(len(record_list))\n",
    "print(len(record_list_removed))\n",
    "\n",
    "#Writte the records in a new file\n",
    "with open(output_file_path, \"wt\") as output:\n",
    "    for record in record_list:\n",
    "        if record in record_list_removed:\n",
    "            output.write(record[0] + \"\\n\")\n",
    "\n",
    "            sequence_split = re.findall(\".{1,60}\", record[1])\n",
    "            for row in sequence_split:\n",
    "                output.write(row + \"\\n\")\n",
    "\n",
    "#Write the duplicate records in a new file\n",
    "with open(\"./duplicate_records\", \"wt\") as duplicate_records_file:\n",
    "    for record in record_list:\n",
    "        if record not in record_list_removed:\n",
    "            duplicate_records_file.write(record[0] + \"\\n\")\n",
    "\n",
    "            sequence_split = re.findall(\".{1,60}\", record[1])\n",
    "            for row in sequence_split:\n",
    "                duplicate_records_file.write(row + \"\\n\")\n",
    "\n",
    "# Print the removed number of records\n",
    "if records_equal_count == 1:\n",
    "    print(\"%d record with the same sequence has been removed\" % records_equal_count)\n",
    "else:\n",
    "    print(\"%d records with the same sequence have been removed\" % records_equal_count)\n",
    "\n",
    "if records_contained_count == 1:\n",
    "    print(\"%d fragment records has been removed\" % records_contained_count)\n",
    "else:\n",
    "    print(\"%d fragment records have been removed\" % records_contained_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative API download\n",
    "\n",
    "# Global imports\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# Script information - Written in Python 3.9.12 - April 2023\n",
    "__author__ = \"Guillermo Carrillo Martín\"\n",
    "__maintainer__ = \"Guillermo Carrillo Martín\"\n",
    "__email__ = \"guillermo.carrillo@upf.edu\"\n",
    "\n",
    "\"\"\"\n",
    "This script creates a protein database in fasta format using the UniParc \n",
    "archive, a non redundand database that contains all the proteins annotated \n",
    "using ENSEMBL, NCBI and other resouces. The search is focus for a specific \n",
    "taxonomic group (indicated using the NCBI tax ID) and can be reduced to \n",
    "a certain group of genes, indicated in a list. Other spcecificities, as \n",
    "the descripion of the protein header, can be seen in the README.md file.\n",
    "\"\"\"\n",
    "\n",
    "#0. Variables needed\n",
    "gene_list_path = \"./gene_list/enamelome_JK.txt\" # The path to the list of genes\n",
    "db_name = \"proboscidea_db.fasta\" # The name of the database file\n",
    "tax_id = 9779 # The tax ID of the organism used to search in UniParc\n",
    "\n",
    "#1. Create the gene list\n",
    "if \"gene_list_path\" in globals():\n",
    "    gene_list = []\n",
    "\n",
    "    with open(gene_list_path, \"rt\") as gene_list_file: #Open gene list file\n",
    "        for gene in gene_list_file:\n",
    "            gene_list.append(gene[0:-1]) #Append the genes in the file to a list\n",
    "\n",
    "#2. Create a taxID list of taxonomic descendent species contained in the taxID\n",
    "\n",
    "\"\"\"\n",
    "In Uniprot, all the proteins with the same sequence have the same unic\n",
    "identifier even they belong to different species. So, at the end, if a \n",
    "protein is conserved throught many species it appears in the database as\n",
    "one record (and the proper unic idenifier) with many species associated.\n",
    "Because of this, sometimes the program retrieves in a recorda random specie \n",
    "that is not from our linage. To fix this error, we create a list with all the \n",
    "downstream taxID to evaluate if the association belongs or not to the correct\n",
    "linage. If it founds a missmatch, in the header there will be the following\n",
    "modifications:\n",
    "\n",
    "OS= taxID corresponging name (for example, Proboscidea)\n",
    "OX= taxID number (for example, 9779)\n",
    "\"\"\"\n",
    "\n",
    "# 2.1 Build API link for the taxonomy download in uniprot\n",
    "url_tax_id_descendent = \"https://rest.uniprot.org/taxonomy/stream?format=list&query=%28%28ancestor%3A{}%29%29\".format(str(tax_id))\n",
    "\n",
    "tax_id_download = str(requests.get(url_tax_id_descendent).text)\n",
    "tax_id_list = tax_id_download.split(\"\\n\")\n",
    "tax_id_list.pop() #Remove the last element of the list (is empty)\n",
    "\n",
    "tax_id_list = [int(taxid) for taxid in tax_id_list] #Convert the string into integers\n",
    "\n",
    "\n",
    "# 2.2 taxID organism name\n",
    "\n",
    "url_org_name = \"https://rest.uniprot.org/taxonomy/stream?format=json&query=%28%28tax_id%3A{}%29%29\".format(str(tax_id))\n",
    "\n",
    "org_name_str = str(requests.get(url_org_name).text)\n",
    "org_name_dic = json.loads(org_name_str)\n",
    "\n",
    "org_name = org_name_dic[\"results\"][0][\"scientificName\"]\n",
    "\n",
    "\n",
    "#3. Convert JSON format to FASTA\n",
    "\n",
    "def json_to_fasta(record, gene_name=0):\n",
    "\n",
    "    #header creator\n",
    "    header = \">\" + record[\"uniParcCrossReferences\"][0][\"database\"] + \"|\" + record[\"uniParcId\"] + \"|\" + \\\n",
    "    record[\"uniParcCrossReferences\"][0][\"lastUpdated\"]+ \" \"\n",
    "    \n",
    "    if \"proteinName\" in record[\"uniParcCrossReferences\"][0]:\n",
    "        header += record[\"uniParcCrossReferences\"][0][\"proteinName\"] + \" \"\n",
    "\n",
    "    if \"organism\" in record[\"uniParcCrossReferences\"][0]:          \n",
    "        if record[\"uniParcCrossReferences\"][0][\"organism\"][\"taxonId\"] not in tax_id_list:\n",
    "            header += \"OS=\" + org_name + \" \" + \\\n",
    "            \"OX=\" + str(tax_id) + \" \"\n",
    "        else:\n",
    "            header += \"OS=\" + record[\"uniParcCrossReferences\"][0][\"organism\"][\"scientificName\"] + \" \" + \\\n",
    "            \"OX=\" + str(record[\"uniParcCrossReferences\"][0][\"organism\"][\"taxonId\"]) + \" \"\n",
    "                \n",
    "    if gene_name != 0:\n",
    "        header += \"GN=\" + gene_name\n",
    "    elif gene_name == 0:\n",
    "        if \"geneName\" in record[\"uniParcCrossReferences\"][0]:\n",
    "            header += \"GN=\" + record[\"uniParcCrossReferences\"][0][\"geneName\"]\n",
    "\n",
    "    #sequence creator\n",
    "    sequence = record[\"sequence\"][\"value\"]\n",
    "\n",
    "    return (header, sequence)\n",
    "\n",
    "#4. Programmatic acces to UniProtKb through the UniProt API  \n",
    "\n",
    "#remove the previous database with the same name\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    os.remove(db_name)\n",
    "\n",
    "#Programatic acces setup\n",
    "\n",
    "re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
    "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
    "session = requests.Session()\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def get_next_link(headers):\n",
    "    if \"Link\" in headers:\n",
    "        match = re_next_link.match(headers[\"Link\"])\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "def get_batch(batch_url):\n",
    "    while batch_url:\n",
    "        response = session.get(batch_url)\n",
    "        response.raise_for_status()\n",
    "        total = response.headers[\"x-total-results\"]\n",
    "        yield response, total\n",
    "        batch_url = get_next_link(response.headers)\n",
    "\n",
    "#Iteratet throught the list of genes and create the db fasta file to append the protein records\n",
    "\n",
    "if \"gene_list_path\" in globals():\n",
    "    \n",
    "    #API URL needed to download the proper sequences\n",
    "    url_api = \"https://rest.uniprot.org/uniparc/search?compressed=false&format=json&query=%28%28taxonomy_id%3A{}%29%20AND%20%28\".format(str(tax_id))\n",
    "    \n",
    "    for gene in gene_list:\n",
    "        url_api += \"%28gene%3A{}%29\".format(gene) #Gene indicator\n",
    "\n",
    "        if gene != gene_list[-1]:\n",
    "            url_api += \"%20OR%20\" #OR operator\n",
    "        \n",
    "    url_api += \"%29%29&size=100\" #End of the URL\n",
    "\n",
    "progress = 0\n",
    "\n",
    "with open(db_name, \"wt\") as file:\n",
    "    for batch, total in get_batch(url_api):\n",
    "        if int(total) > 0: #If there are more than one record found\n",
    "            json_retrieve = batch.json()\n",
    "            for protein in json_retrieve[\"results\"]:\n",
    "                retrive = json_to_fasta(protein)\n",
    "                file.write(retrive[0] + \"\\n\")\n",
    "                        \n",
    "                sequence_split = re.findall(\".{1,60}\", retrive[1])\n",
    "                for row in sequence_split:\n",
    "                    file.write(row + \"\\n\")\n",
    "            progress += len(json_retrieve[\"results\"])\n",
    "            print(f'{progress} / {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metadata\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "#Global variables\n",
    "database_path = \"proboscidea_db.fasta\"\n",
    "metadata_path = \"metadata.txt\"\n",
    "\n",
    "#Current time and date\n",
    "now = datetime.now()\n",
    "dat_time = now.strftime(\"%d/%m/%Y - %H:%M:%S\")\n",
    "\n",
    "#Calculate number of records and records with a gene\n",
    "record_num = os.popen(f\"grep -E -c '^>' {database_path}\").read()\n",
    "record_with_gene = os.popen(f\"grep -E '^>' {database_path} | grep -c 'GN='\").read()\n",
    "\n",
    "warnings = 0\n",
    "\n",
    "#Append the metadata information in a file\n",
    "with open(metadata_path, \"wt\") as metadata:\n",
    "    metadata.write(\"## METADATA for the database '{}'\".format(database_path) + \"\\n\" + \\\n",
    "                   \"{}\".format(dat_time) + \"\\n\" + \"\\n\")\n",
    "                   \n",
    "    metadata.write(\"# WARNINGS\\n\")\n",
    "    if record_num != record_with_gene:\n",
    "        warnings += 1\n",
    "        metadata.write(\"Check gene names (GN=)\\n\")\n",
    "    \n",
    "    if warnings == 0:\n",
    "        metadata.write(\"No one\\n\")\n",
    "\n",
    "    metadata.write(\"\\n# Stats about the protein records\\n\" + \\\n",
    "                    \"number_of_records: {}\".format(record_num) + \\\n",
    "                    \"number_of_records_with_gene_(GN=): {}/{}\".format(record_with_gene[:-1],record_num) + \"\\n\")\n",
    "                    \n",
    "    metadata.write(\"# Stats about the genes found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bash oneline\n",
    "\n",
    "import os\n",
    "\n",
    "database_stats_path = \"example.csv\"\n",
    "\n",
    "os.system(f\"echo 'Database,Count' > {database_stats_path}\")\n",
    "os.system(\"cut -f2 -d, metadata_proboscidea/records_info.csv | tail -n +2 | sort | uniq -c | while read line; do words=($line); echo ${words[1]},${words[0]}; done\" + f\"| sort -k 2 -n -r -t , >> {database_stats_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New metadata\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "# Script information - Written in Python 3.9.12 - May 2023\n",
    "__author__ = \"Guillermo Carrillo Martín\"\n",
    "__maintainer__ = \"Guillermo Carrillo Martín\"\n",
    "__email__ = \"guillermo.carrillo@upf.edu\"\n",
    "\n",
    "\"\"\"\n",
    "This script reads a protein multi-fasta database and creates a folder with metadata \n",
    "information. The seven files created by the script are:\n",
    "\n",
    "1. metadata_readme.txt; A text file that contains a summary about all the metadata\n",
    "    retrieved from the database. It also shows the paths to all the other files.\n",
    "\n",
    "2. records_info.csv; A csv file that contains all the information present in each \n",
    "    record header. To see a detailed description of the header information, see \n",
    "    the README.md file.\n",
    "\n",
    "3. databases_employed.csv; A csv file that contains the number of the different\n",
    "    sources (databases) used to build the multi-fasta. \n",
    "\n",
    "4. genes_retrieved.csv; A csv file that contains the number of genes retrieved.\n",
    "\n",
    "5. genes_NOT_retrieved.txt; A text file that contains the genes not retrieved. This\n",
    "    file is only created if the list of genes used to build the multi-fasta is \n",
    "    specified.\n",
    "\n",
    "6. species_retrieved.csv; A csv file that contains the number of species retrieved, \n",
    "    indicating the scientific name and the TaxID.\n",
    "\n",
    "7. species_genes.csv; A csv file that that shows the number of genes retrieved \n",
    "    for each species.\n",
    "\n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "The script has two main sections:\n",
    "\n",
    "1. Calculate all the metadata information and stats from the multi-fasta. Also, \n",
    "    creates all the files previously mentioned except the metadata_readme.txt file.\n",
    "\n",
    "2. Create the metadata_readme.txt file and print there all the information previously\n",
    "    calculated. \n",
    "\"\"\"\n",
    "\n",
    "#0. Set the arguments to run the program from the command line (parser)\n",
    "parser = argparse.ArgumentParser(description=\"A script that creates metadata information for a database created by uniparc_db.py\")\n",
    "parser.add_argument(\"--input_path\", dest=\"input_path\", type=str, help=\"\", required=True, nargs=\"?\")\n",
    "parser.add_argument(\"--output_folder\", dest=\"folder_path\", type=str, help=\"\", required=True, nargs=\"?\")\n",
    "parser.add_argument(\"--genes\", dest=\"gene_list\", type=str, help=\"(nor required)\", required=False, nargs=\"?\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "database_path = args.input_path\n",
    "metadata_folder_path = args.folder_path\n",
    "gene_list_path = args.gene_list\n",
    "\n",
    "#1. Creating stats\n",
    "#Create metadata folder\n",
    "if os.path.exists(metadata_folder_path):\n",
    "    shutil.rmtree(metadata_folder_path)\n",
    "\n",
    "os.mkdir(metadata_folder_path)\n",
    "\n",
    "#Current time and date\n",
    "now = datetime.now()\n",
    "dat_time = now.strftime(\"%d/%m/%Y - %H:%M:%S\")\n",
    "\n",
    "#Create the gene list if the path is specified\n",
    "if gene_list_path != None:\n",
    "\n",
    "    #a. Append in a list the genes in the gene list\n",
    "    gene_list = []\n",
    "    with open(gene_list_path, \"rt\") as gene_list_file: #Open gene list file\n",
    "        for gene in gene_list_file:\n",
    "            gene_list.append(gene[0:-1].strip())\n",
    "\n",
    "#1.1 Stats about the protein records\n",
    "#1.1.1 Number of records; records with gene name, scientfic name and TaxI\n",
    "record_num = os.popen(f\"grep -E -c '^>' {database_path}\").read() \n",
    "record_with_gene = os.popen(f\"cat {database_path} | grep -E '^>' | grep -c 'GN='\").read()\n",
    "record_with_taxname = os.popen(f\"cat {database_path} | grep -E '^>' | grep -c 'OS='\").read()\n",
    "record_with_taxid = os.popen(f\"cat {database_path} | grep -E '^>' | grep -c 'OX='\").read()\n",
    "\n",
    "#1.1.2 Create records_info.csv\n",
    "#a. Function that retrieves a header tag value using a regular expression\n",
    "def retrieve_tag(regular_expression, header):\n",
    "    tag_match = re.search(r\"\" + regular_expression, header)\n",
    "    \n",
    "    if tag_match:\n",
    "        tag = tag_match.group(1)\n",
    "    \n",
    "    elif not tag_match:\n",
    "        tag = \"\"\n",
    "\n",
    "    return tag\n",
    "\n",
    "#b. Iterate through the multi-fasta \n",
    "with open(database_path, \"rt\") as database_file:\n",
    "    records = [] #\n",
    "\n",
    "    for line in database_file:  \n",
    "        if line.startswith(\">\"): #For each header in the multi-fasta\n",
    "            #Retrieve the values of the following parameters:\n",
    "            \n",
    "            upi = retrieve_tag(\"\\|([^|]+)\\|\", line)  # Unic identifier\n",
    "            database = retrieve_tag(\"^>([^|]+)\", line) # Database\n",
    "            gene = retrieve_tag(\"GN=(\\w+)\", line).upper() # Gene\n",
    "            species = retrieve_tag(\"OS=(.*?)\\sOX=\", line) # Species\n",
    "            taxid = retrieve_tag(\"OX=(\\w+)\", line) # TaxID\n",
    "            update = retrieve_tag(\"\\|([\\d-]+)\", line) # Update date\n",
    "\n",
    "            header_info = \"{},{},{},{},{},{}\".format(upi,database,gene,species,taxid,update)\n",
    "            records.append(header_info)\n",
    "\n",
    "#c. Print all the information in a csv file        \n",
    "with open(metadata_folder_path + \"/records_info.csv\", \"wt\") as records_info_file:\n",
    "    records_info_file.write(\"Unic Identifier,Database,Gene,Species,TaxID,Last Update\\n\")\n",
    "    for header in records:\n",
    "        records_info_file.write(header + \"\\n\")\n",
    "\n",
    "#d. Read the record csv file as a Panda dataframe for future calculations\n",
    "record_df = pd.read_csv(\"metadata_proboscidea/records_info.csv\")\n",
    "\n",
    "#1.2 Stats about the employed databases\n",
    "databases_count_sr = record_df[\"Database\"].value_counts() #A Panda serie counting the values of \"Databases\"\n",
    "databases_count_sr.to_csv(metadata_folder_path + \"/databases_employed.csv\", index_label=\"Database\", header=[\"Count\"])\n",
    "\n",
    "databases_num = databases_count_sr.count()\n",
    "\n",
    "#1.3 Stats about the genes found\n",
    "#1.3.1 genes retrieved\n",
    "record_df[\"Gene\"] = record_df[\"Gene\"].str.upper()\n",
    "gene_count_sr = record_df[\"Gene\"].value_counts() #A Panda serie counting the values of \"Databases\"\n",
    "gene_count_sr.to_csv(metadata_folder_path + \"/genes_retrieved.csv\", index_label=\"Gene\", header=[\"Count\"])\n",
    "\n",
    "genes_num = gene_count_sr.count()\n",
    "\n",
    "# 1.3.2 Genes not retrieved, if we are using a gene list\n",
    "if gene_list_path != None:\n",
    "\n",
    "    #a. Append in a list the genes found\n",
    "    gene_found_ls = list(record_df[\"Gene\"].unique())\n",
    "\n",
    "    #b. Compare the genes found with the gene list and create genes not found list\n",
    "    gene_not_found_ls = []\n",
    "    for gene in gene_list:\n",
    "        if gene not in gene_found_ls:\n",
    "            gene_not_found_ls.append(gene)\n",
    "\n",
    "    #c. Write the genes not retrieved in a text file\n",
    "    with open(metadata_folder_path + \"/genes_NOT_retrieved.txt\", \"wt\") as genes_not_retrieved:\n",
    "        for gene in gene_not_found_ls:\n",
    "            genes_not_retrieved.write(gene + \"\\n\")\n",
    "\n",
    "#1.4 Stats about the species in the database\n",
    "#1.4.1 Species retrieved\n",
    "species_count_sr = record_df[[\"Species\", \"TaxID\"]].value_counts() #A Panda serie counting the values of \"Databases\"\n",
    "species_count_sr.to_csv(metadata_folder_path + \"/species_retrieved.csv\", index_label=[\"Species\", \"TaxID\"], header=[\"Count\"])\n",
    "\n",
    "species_num = species_count_sr.count()\n",
    "\n",
    "#1.4.2 Genes per species\n",
    "\n",
    "species_genes = {}\n",
    "\n",
    "\n",
    "\n",
    "with open(database_path, \"rt\") as database:\n",
    "    species_genes_dic = {}\n",
    "\n",
    "    for line in database:\n",
    "        if line.startswith(\">\"):\n",
    "            \n",
    "            #1.6.2 Create species + genes\n",
    "            specie_match = re.search(r\"OS=(.*?)\\sOX=\", line)\n",
    "            taxid_match = re.search(r\"OX=(\\w+)\", line)\n",
    "            gene_match = re.search(r\"GN=(\\w+)\", line)\n",
    "            \n",
    "            if specie_match and taxid_match and gene_match:\n",
    "                species = specie_match.group(1)\n",
    "                taxid = taxid_match.group(1)\n",
    "                gene = gene_match.group(1).upper()\n",
    "\n",
    "                trio = \"{},{},{},\".format(species,taxid,gene)\n",
    "                if trio not in species_genes_dic:\n",
    "                    species_genes_dic[trio] = 1\n",
    "                elif trio in species_genes_dic:\n",
    "                    species_genes_dic[trio] += 1\n",
    "    \n",
    "    #A list with the species found\n",
    "    species_found_ls = list(record_df[\"Species\"].unique())\n",
    "\n",
    "\n",
    "    #If there is a gene list, fill the genes not found in a specie with the gene list\n",
    "    if gene_list_path != None:\n",
    "\n",
    "        for species in species_found_ls:\n",
    "            for gene in gene_list:\n",
    "                if species + gene + \",\" not in species_genes_dic:\n",
    "                    species_genes_dic[species + gene] = \"0\"  \n",
    "\n",
    "    #If\n",
    "    if gene_list_path == None:\n",
    "        \"\"     \n",
    "\n",
    "#1.6.2 Create species and genes file\n",
    "with open(metadata_folder_path + \"/species_genes.csv\", \"wt\") as species_genes:\n",
    "    species_genes.write(\"Species,TaxID,Gene,Count\\n\")\n",
    "    for header, count in species_genes_dic.items():\n",
    "        species_genes.write(header + str(count) + \"\\n\")\n",
    "\n",
    "#sort csv first by species name and then by count of genes\n",
    "species_genes_table = pd.read_csv(metadata_folder_path + \"/species_genes.csv\")\n",
    "species_genes_table.sort_values([\"Species\", \"Count\"], axis=0,ascending=[True, False], inplace=True)\n",
    "\n",
    "species_genes_table.to_csv(metadata_folder_path + \"/species_genes.csv\", index=False)\n",
    "\n",
    "#2. Append the metadata information in a file\n",
    "with open(metadata_folder_path + \"/metadata_readme.txt\", \"wt\") as metadata:\n",
    "    metadata.write(\"## METADATA for the database '{}'\".format(database_path) + \"\\n\" + \\\n",
    "                   \"{}\".format(dat_time) + \"\\n\" + \"\\n\")\n",
    "\n",
    "    #2.1 WARNINGS\n",
    "\n",
    "    warnings = 0 \n",
    "                   \n",
    "    metadata.write(\"# WARNINGS\\n\")\n",
    "    if record_num != record_with_gene:\n",
    "        warnings += 1\n",
    "        metadata.write(\"Check gene names (GN=)\\n\")\n",
    "    \n",
    "    if record_num != record_with_taxname:\n",
    "        warnings += 1\n",
    "        metadata.write(\"Check taxa names (OS=)\\n\")\n",
    "\n",
    "    if record_num != record_with_taxid:\n",
    "        warnings += 1\n",
    "        metadata.write(\"Check taxIDs (OX=)\\n\")\n",
    "\n",
    "    if gene_list_path != None:\n",
    "        if len(gene_found_ls) != len(gene_list):\n",
    "            warnings += 1\n",
    "            metadata.write(\"{} genes not found\\n\".format((len(gene_list)) - (len(gene_found_ls))))\n",
    "\n",
    "    if warnings == 0:\n",
    "        metadata.write(\"No warnings\\n\")\n",
    "\n",
    "    #2.2 RECORDS STATS\n",
    "    metadata.write(\"\\n# Stats about the protein records\\n\" + \\\n",
    "                    \"records: {}\".format(record_num) + \\\n",
    "                    \"records_with_gene: {}/{}\".format(record_with_gene[:-1],record_num) + \\\n",
    "                    \"records_with_taxname: {}/{}\".format(record_with_taxname[:-1],record_num) + \\\n",
    "                    \"records_with_taxid: {}/{}\".format(record_with_taxid[:-1],record_num) + \n",
    "                    \"all_records_table_path: {}\\n\".format(metadata_folder_path + \"/records_info.csv\"))\n",
    "\n",
    "    #2.3 DATATBASE STATS\n",
    "    metadata.write(\"\\n# Stats about the employed databases\\n\" + \\\n",
    "                    \"databases_employed: {}\\n\".format(databases_num) + \\\n",
    "                    \"databases_employed_path: {}\\n\\n\".format(metadata_folder_path + \"/databases_employed.csv\"))\n",
    "\n",
    "    #2.4 GENE STATS  \n",
    "    if gene_list_path != None:            \n",
    "        metadata.write(\"# Stats about the genes found\\n\" + \\\n",
    "                        \"genes_retrieved: {}/{}\\n\".format(genes_num,str(len(gene_list))) + \\\n",
    "                        \"genes_retrieved_path: {}\\n\".format(metadata_folder_path + \"/genes_retrieved.csv\") + \\\n",
    "                        \"genes_not_retrieved_path: {}\\n\".format(metadata_folder_path + \"/genes_NOT_retrieved.txt\"))\n",
    "    \n",
    "    elif gene_list_path == None:\n",
    "        metadata.write(\"# Stats about the genes found\\n\" + \\\n",
    "                        \"genes_retrieved: {}\\n\".format(genes_num) + \\\n",
    "                        \"genes_retrieved_path: {}\\n\".format(metadata_folder_path + \"/genes_retrieved.csv\"))\n",
    "\n",
    "    #2.5 SPECIES STATS\n",
    "    metadata.write(\"\\n# Stats about the species in the database\\n\" + \\\n",
    "                    \"species_retrieved: {}\\n\".format(species_num) + \\\n",
    "                    \"species_retrieved_path: {}\\n\".format(metadata_folder_path + \"/species_retrieved.csv\") + \\\n",
    "                    \"species_and_genes_path: {}\".format(metadata_folder_path + \"/species_genes.csv\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
